{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc405543",
   "metadata": {},
   "source": [
    "# 1. Load files and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bbd4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.1 load files, and add tipDefault columns for tips prediction#@title Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import normalize\n",
    "import datetime as dt\n",
    "\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e73d8b2",
   "metadata": {},
   "source": [
    "## 1.1 load files, and add tipDefault columns for tips prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"share_ride_data.csv\")\n",
    "df = df.sample(frac=0.02, random_state=42)\n",
    "# create binary feature tipPay\n",
    "df[\"tipPay\"] = (df['Tip'] != 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0e78e",
   "metadata": {},
   "source": [
    "## 1.2 Features engeneering part 1: OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bdc06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use OneHotEncoding add features describe day of the week; time of the day\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df[\"pickupHour\"] = pd.to_datetime(df[\"Trip Start Timestamp\"], format='%m/%d/%Y %I:%M:%S %p').dt.hour\n",
    "df[\"pickDayofweek\"] = pd.to_datetime(df[\"Trip Start Timestamp\"], format='%m/%d/%Y %I:%M:%S %p').dt.weekday\n",
    "df.drop_duplicates(subset=[\"pickupHour\",\"pickDayofweek\",\"Pickup Community Area\",\"Dropoff Community Area\",'Pickup Centroid Latitude','Pickup Centroid Longitude'], inplace= True, keep='last')\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = encoder.fit_transform(df[[\"pickupHour\",\"pickDayofweek\",\"Pickup Community Area\",\"Dropoff Community Area\"]])\n",
    "onehot_encoded_frame = pd.DataFrame(onehot_encoded,columns = encoder.get_feature_names(['hourofday', 'dayofweek','pickuparea','dropoffarea']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635a0e0",
   "metadata": {},
   "source": [
    "## 1.3 Examine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c467567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of statistics\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11961ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique value counts of each column, small number could be numeric category data\n",
    "pd.DataFrame(df.unique(), columns=['Unique Values']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195d0b7",
   "metadata": {},
   "source": [
    "I briefly examine the summary statistics here:\n",
    "\n",
    "Features have largely heterogeneous distributions.\n",
    "If I use linear methods such as logistic regression, I need to normalize variables\n",
    "The labels are all zeros and ones. The mean is 0.16, implying that only 16% of the data are labeled default=0.\n",
    "This is an imbalanced binary classification problem.\n",
    "Timestamp variables seem to be continuous because of the high number of unique values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178ce9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine data districution \n",
    "columnsName = df.columns\n",
    "for i in range(0,14):\n",
    "  plt.figure(figsize=(15,5))\n",
    "  plt.hist(df.iloc[:,i],bins=150,ec='w');plt.title(columnsName[i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba816b",
   "metadata": {},
   "source": [
    "trip timestamp have seasonality\n",
    "Trip miles, Additional charges, Trip total heavy right skewed. use logarithms deal with tese features\n",
    "need to normalize data because the distributions are highly hetergeneous. Otherwise, models such as logistic regression may never converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad576b",
   "metadata": {},
   "source": [
    "## 1.4 Examine multicollinearity\n",
    "There's no too much for me to do because the dataset is very structured itself: there are no missing values; variables are well-defined; all values are integers or floats; I have no context of the columns\n",
    "So, I mainly consider the possibility of redundant features. In particular, multicollinearity.\n",
    "Multicollinearity can cause very unstable model performance, especially for ordinary linear regression, because X â€² X is not full-rank and is not invertible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap correlation between features\n",
    "plt.figure(figsize=(15,12))\n",
    "corr_before = df.corr()\n",
    "sns.heatmap(corr_before, vmin=-1, vmax=1, cmap=\"RdBu_r\", lw=.1)\n",
    "plt.title('Correlation heatmap of variables');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a05ecf",
   "metadata": {},
   "source": [
    "I plot the correlation of each variables in a heatmap:\n",
    "\n",
    "Trip Toal calculate by fair + tip, fair and tip values won't have predcting power if Trip Toal is already included\n",
    "trip total and trip mile are linear correlated, only use trip total for tipDefault prediction\n",
    "# 2. model experiment\n",
    "## 2.1 features for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4555331",
   "metadata": {},
   "outputs": [],
   "source": [
    "fea = df[['Pickup Centroid Latitude','Pickup Centroid Longitude']]\n",
    "# combine original features and onehot_encoded_frame\n",
    "features = pd.concat([onehot_encoded_frame.reset_index(),fea.reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01d7e8",
   "metadata": {},
   "source": [
    "## 2.2 Preparation\n",
    "Normalize data because the distributions are highly hetergeneous. Otherwise, models such as logistic regression may never converge.\n",
    "Split the training data into training/validation sets for cross validation (hyperparameter tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "X = normalize(features)\n",
    "# label\n",
    "y = df[\"tipPay\"]\n",
    "\n",
    "# split into 0.8 training dataset and 0.2 test dataset\n",
    "X_t, X_test, y_t, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# #split into 0.6 training dataset 0.2 validation dataset \n",
    "X_train, X_val, y_train,y_val = train_test_split(X_t, y_t, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc805cc2",
   "metadata": {},
   "source": [
    "## 2.3 Logistic regression\n",
    "use f1 score\n",
    "since there are lot's of samples that are default=0 than default=1, then precision might be useful: precision does not include number of true negative in its calculation, so not influenced by imbalance\n",
    "use F1 score to incorporates both the quality of predictions and compleness of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84243863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ac14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression with normalized features\n",
    "logit_1 = LogisticRegression()\n",
    "logit_1.fit(X_train, y_train)\n",
    "y_pred_1 = logit_1.predict(X_val)\n",
    "\n",
    "training_accuracy = logit_1.score(X_train, y_train)\n",
    "print('Training accuracy:', round(logit_1.score(X_train, y_train),3))\n",
    "print('Validation accuracy:', round(logit_1.score(X_val, y_val),3))\n",
    "print('Validation precision:', round(recall_score(y_val, y_pred_1),3))\n",
    "print('Validation recall:', round(precision_score(y_val, y_pred_1),3))\n",
    "print('Validation f1 score:', round(f1_score(y_val, y_pred_1),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee281c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusio matrix\n",
    "plot_confusion_matrix(logit_1, X_val, y_val, values_format='d')\n",
    "plt.title('vanilla Logistic regression: confusion matrix of validation data');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47105778",
   "metadata": {},
   "source": [
    "## 2.3 Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb911fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest with normalized features\n",
    "forest_1 = RandomForestClassifier(random_state=0)\n",
    "forest_1.fit(X_train, y_train)\n",
    "y_pred_f = forest_1.predict(X_val)\n",
    "\n",
    "print('Training accuracy:', round(forest_1.score(X_train, y_train),3))\n",
    "print('Validation accuracy:', round(forest_1.score(X_val, y_val),3))\n",
    "print('Validation precision:', round(recall_score(y_val, y_pred_f),3))\n",
    "print('Validation recall:', round(precision_score(y_val, y_pred_f),3))\n",
    "print('Validation f1 score:', round(f1_score(y_val, y_pred_f),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusio matrix\n",
    "plot_confusion_matrix(forest_1, X_val, y_val, values_format='d')\n",
    "plt.title('vanilla Random forest: confusion matrix of validation data');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff2cbf8",
   "metadata": {},
   "source": [
    "# 3. model improvement\n",
    "## 3.1 Re-weight Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-weight logistic regression with normalized features\n",
    "logit_2 = LogisticRegression(class_weight='balanced')\n",
    "logit_2.fit(X_train, y_train)\n",
    "y_pred_2 = logit_2.predict(X_val)\n",
    "\n",
    "training_accuracy = logit_2.score(X_train, y_train)\n",
    "print('Training accuracy:', round(logit_2.score(X_train, y_train),3))\n",
    "print('Validation accuracy:', round(logit_2.score(X_val, y_val),3))\n",
    "print('Validation precision:', round(recall_score(y_val, y_pred_2),3))\n",
    "print('Validation recall:', round(precision_score(y_val, y_pred_2),3))\n",
    "print('Validation f1 score:', round(f1_score(y_val, y_pred_2),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04762d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusio matrix\n",
    "plot_confusion_matrix(logit_2, X_val, y_val, values_format='d')\n",
    "plt.title('re-weight Logistic regression: confusion matrix of validation data');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcdf6b0",
   "metadata": {},
   "source": [
    "## 3.2 Re-weight Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest with normalized features\n",
    "forest_2 = RandomForestClassifier(random_state=0,class_weight='balanced')\n",
    "forest_2.fit(X_train, y_train)\n",
    "y_pred_r2 = forest_2.predict(X_val)\n",
    "\n",
    "print('Training accuracy:', round(forest_2.score(X_train, y_train),3))\n",
    "print('Validation accuracy:', round(forest_2.score(X_val, y_val),3))\n",
    "print('Validation precision:', round(recall_score(y_val, y_pred_r2),3))\n",
    "print('Validation recall:', round(precision_score(y_val, y_pred_r2),3))\n",
    "print('Validation f1 score:', round(f1_score(y_val, y_pred_r2),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9deaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusio matrix\n",
    "plot_confusion_matrix(forest_2, X_val, y_val, values_format='d')\n",
    "plt.title('re-weight Random forest: confusion matrix of validation data');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e38dea",
   "metadata": {},
   "source": [
    "# 4. Hyperparameter tuning with cross validation\n",
    "## 4.1 Hyperparameter tuning Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7489cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for logistic regression\n",
    "parameters = {'C': np.linspace(.001,1,20)}\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "clf = GridSearchCV(model, parameters, cv=5, n_jobs=-1)\n",
    "clf.fit(X_t, y_t)\n",
    "best_C = np.linspace(.001,1,20)[clf.cv_results_['rank_test_score'].argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1302d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model w/ best C\n",
    "logit_best = LogisticRegression(C=best_C, class_weight='balanced')\n",
    "logit_best.fit(X_t, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966990a6",
   "metadata": {},
   "source": [
    "## 4.2 Hyperparameter tuning Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6db3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for random forest \n",
    "parameters = {'max_depth': np.arange(2,5), 'min_samples_split': np.arange(2,50,10), 'min_samples_leaf': np.arange(1,50,10)}\n",
    "# parameters = {'max_depth': np.arange(2,4), 'min_samples_split': np.arange(2,13,10)}\n",
    "model = RandomForestClassifier(class_weight='balanced')\n",
    "clf = GridSearchCV(model, parameters, cv=5, n_jobs=-1)\n",
    "clf.fit(X_t, y_t)\n",
    "best_max_depth = clf.best_params_['max_depth']\n",
    "best_min_samples_split = clf.best_params_['min_samples_split']\n",
    "best_min_samples_leaf = clf.best_params_['min_samples_leaf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beded55",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_depth = clf.best_params_['max_depth']\n",
    "best_min_samples_split = clf.best_params_['min_samples_split']\n",
    "best_min_samples_leaf = clf.best_params_['min_samples_leaf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e03349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model w/ best C\n",
    "forest_best = RandomForestClassifier(class_weight='balanced', max_depth= best_max_depth, min_samples_split= best_min_samples_split, min_samples_leaf = best_min_samples_leaf)\n",
    "forest_best.fit(X_t, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a29cda4",
   "metadata": {},
   "source": [
    "## 4.3 Evaluation for best models\n",
    "### 4.3.1 Evaluation for best Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f47e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same feature engineering has applied in earlier part the test data\n",
    "# logistic regression prediction\n",
    "y_pred_logit_best = logit_best.predict(X_test)\n",
    "y_test_prob_logit_best =logit_best.predict_proba(X_test)\n",
    "print('Train Accuracy:', round(logit_best.score(X_t, y_t),3))\n",
    "print('Test Accuracy:', round(logit_best.score(X_test, y_test),3))\n",
    "print('Test Precision:', round(recall_score(y_test, y_pred_logit_best),3))\n",
    "print('Test Recall:', round(precision_score(y_test, y_pred_logit_best),3))\n",
    "print('Test F1 score:', round(f1_score(y_test, y_pred_logit_best),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad009c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusio matrix\n",
    "plot_confusion_matrix(logit_best, X_test, y_test, values_format='d')\n",
    "plt.title('Best logistic regression: confusion matrix of test data');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0c9dd",
   "metadata": {},
   "source": [
    "### 4.3.2 Evaluation for best Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716266c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_forest_best = forest_best.predict(X_test)\n",
    "y_test_prob_forest_best = forest_best.predict_proba(X_test)\n",
    "print('Train Accuracy:', round(forest_best.score(X_t, y_t),3))\n",
    "print('Test Accuracy:', round(forest_best.score(X_test, y_test),3))\n",
    "print('Test Precision:', round(recall_score(y_test, y_pred_forest_best),3))\n",
    "print('Test Recall:', round(precision_score(y_test, y_pred_forest_best),3))\n",
    "print('Test F1 score:', round(f1_score(y_test, y_pred_forest_best),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd964a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusio matrix\n",
    "plot_confusion_matrix(forest_best, X_test, y_test, values_format='d')\n",
    "plt.title('Best Random forest: confusion matrix of test data');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c4df8",
   "metadata": {},
   "source": [
    "### 4.3.3 Compare performance of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8edf957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3827b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot roc for comparation\n",
    "# green best random forest, red best logistic regression\n",
    "fpr_1, tpr_1, thresholds_1 = roc_curve(y_test, y_test_prob_logit_best[:,1])\n",
    "plt.plot(fpr_1,tpr_1,color=\"red\", label='best logistic regression')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curve for learners\")\n",
    "fpr_2, tpr_2, thresholds_2 = roc_curve(y_test, y_test_prob_forest_best[:,1])\n",
    "plt.plot(fpr_2,tpr_2, color=\"red\", label='best random forest')\n",
    "plt.plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f37fe9",
   "metadata": {},
   "source": [
    "### 4.3.3 Result\n",
    "Re-weight Logistic regression has the best performance base on tradeoff between F1 score and AUC\n",
    "We want a related higer F1 score to ensure we have a prediction with quality and completeness\n",
    "We want a related higer accuracy to ensure our predicting power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7084e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
